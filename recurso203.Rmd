---
title: <span style="color:#686868">**Variables conjuntas**</span>
author: "Métodos y Simulación Estadística"
output:
  html_document:
    toc: no
    toc_depth: 2
    toc_float: yes
    code_folding: hide
    css: style.css
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE, comment = NA)


```

Cuando se analizan dos variables simultáneamente, se forma una **variable aleatoria bivariada** $(X, Y)$, definida sobre un plano. La **probabilidad conjunta** de $(X, Y)$ describe la probabilidad de que ambas variables tomen valores específicos simultáneamente. Esta probabilidad genera una **superficie tridimensional**, como se ilustra en la **Figura 2.15** que corresponde a una normal bivariada.



```{r, echo=FALSE}
# tomada de:
# http://pj.freefaculty.org/R/WorkingExamples/plot-3d-MVNormal-1.R
# library("mvtnorm")
# 
# N=30
# x <- seq(-4,4, length=N)
# y <- seq(-4,4,length=N)
# z <- matrix(0, N, N)
# for (i in 1:N) for (j in 1:N) {
# 	z[i,j]=dmvnorm(c(x[i],y[j]), c(0,0),
# 	matrix(c(1,0.5,0.5,1),2,2))}
# persp(x,y,z,theta=20, phi=15, 
# xlab="x", 
# ylab="y", 
# zlab="f(x,y)",
# scale=TRUE,
# expand=.4,
# axes=TRUE,
# col="#034A94")
```


<br/><br/>
<center>
```{r, echo=FALSE, out.width="80%", fig.align = "center"}
knitr::include_graphics("img/normal_bi.png")
```
**Figura 2.15** Distribución normal bivariada.
</center>
<br/><br/>



</br></br>
<h2>Introducción</h2>

En muchos experimentos, los resultados son influenciados por múltiples variables. Ejemplos comunes incluyen:

- Precio de un producto y su volumen de ventas.

- Tiempo de preparación para un examen y la calificación obtenida.

- Cantidad de arena y cemento en una mezcla de concreto.

- Cantidad de abono aplicado y la producción de una planta.

En estos casos, es necesario emplear una función de densidad que describa la probabilidad conjunta de ambas variables. Esta función caracteriza cómo se comportan **simultáneamente** las variables involucradas. Se puede analizar la relación entre las variables:

- **Continua-continua:** Ambas variables son continuas.

- **Discreta-discreta:** Ambas variables son discretas.

- **Continua-discreta:** Una variable es continua y la otra es discreta.

Esta guía se centra en los casos más representativos:

- **Discreta-discreta**: Ejemplificado mediante tablas de probabilidad conjunta y distribuciones marginales.

- **Continua-continua:** Ilustrado mediante funciones de densidad conjunta y gráficos de superficies.




</br></br>
<h2>Discreto-discreto</h2>


</br></br>
<h3>Función de distribución de probabilidad conjunta</h3>

Cuando **$X$** y **$Y$** son variables aleatorias discretas, la **función de probabilidad conjunta** se define como:
$$
f_{X,Y}(x,y) = P(X = x, Y = y)
$$
Esta función representa la probabilidad de que $X$ tome el valor $x$ y $Y$ tome el valor $y$ simultáneamente.


</br></br>
<h3>Propiedades de la distribución conjunta</h3>

Para ser válida, la función de distribución de probabilidad conjunta debe cumplir con las siguientes características:

1. **Totalidad de la probabilidad:** La suma de todas las probabilidades conjuntas es igual a 1:
$$
\sum_{x=x_{(1)}}^{x_{(n)}} \sum_{y=y_{(1)}}^{y_{(n)}} f_{X,Y}(x,y) = 1
$$
Esto asegura que la probabilidad total cubre todos los resultados posibles.

2. **No negatividad:** Todas las probabilidades deben ser no negativas:
$$
f_{X,Y}(x,y) \geq 0 \quad \text{para todo } x,y
$$
Esta función describe la probabilidad de ocurrencia simultánea de dos eventos discretos. Se suele representar mediante una **tabla de probabilidad conjunta**, donde las filas corresponden a valores de $X$ y las columnas a valores de $Y$.


</br></br>
<div class="caja-ejemplo">
<h3>Ejemplo:</h3>
<p>

Considere las variables:

- **$X$:** Número de fallas de una máquina por día, con $R_X = \{1,2,3\}$.

- **$Y$:** Número de veces que el operario llama al técnico, con $R_Y = \{1,2,3\}$.

La función de probabilidad conjunta \( f_{X,Y}(x,y) \), que modela el número de fallas y el número de llamadas, está dada en la **Tabla 2.7**. En esta, se observa que la probabilidad de que el operario realice una llamada por una falla es 0.05, mientras que la probabilidad de que realice dos llamadas por dos fallas es 0.10.

 

<br/><br/>
<center>
**Tabla 2.7** Distribución conjunta de $X$, $Y$.
</center> 

|    |        |       |  $y$ |      |
|:--:|:------:|:-----:|:----:|:----:|
|    |$f_{X,Y}(x,y)$| 1     |  2   |  3   |
|$x$ |  1     | 0.05  | 0.05 | 0.10 |
|    |  2     | 0.05 | 0.10 | 0.35  |
|    |  3     | 0     | 0.20 | 0.10 |

Los valores de la **Tabla 2.7** son no negativos y cumplen que su suma es igual a 1. Esto se puede verificar con los siguientes códigos en **R**:


<pre>
fxy <- matrix(c(0.05, 0.05, 0.00,
                0.05, 0.10, 0.20,
                0.10, 0.35, 0.10),
              ncol = 3, byrow = FALSE)
colnames(fxy) <- c("Y=1", "Y=2", "Y=3")
rownames(fxy) <- c("X=1", "X=2", "X=3")
print(fxy)

suma_total <- sum(fxy)
print(paste("Suma total: ", suma_total))
</pre>


```{r, eval=FALSE, include=FALSE}
fxy <- matrix(c(0.05, 0.05, 0.00,
                0.05, 0.10, 0.20,
                0.10, 0.35, 0.10),
              ncol = 3, byrow = FALSE)
colnames(fxy) <- c("Y=1", "Y=2", "Y=3")
rownames(fxy) <- c("X=1", "X=2", "X=3")
print(fxy)

suma_total <- sum(fxy)
print(paste("Suma total: ", suma_total))
```

La función de probabilidad conjunta \( f_{X,Y}(x,y) \) se puede representar mediante un gráfico tridimensional, como se muestra en la **Figura 2.16**. En el plano \(X\)-\(Y\), se ubican los valores conjuntos de las variables, mientras que la altura de las rectas sobre cada punto indica la probabilidad correspondiente en el eje \(Z\). El gráfico se puede obtener con los siguientes códigos:


<pre>
library(plot3D)

# Valores de fxy, X y Y proporcionados
fxy <- matrix(c(0.05, 0.05, 0.00,
                0.05, 0.10, 0.20,
                0.10, 0.35, 0.10),
              ncol = 3, byrow = FALSE)
colnames(fxy) <- c("Y=1", "Y=2", "Y=3")
rownames(fxy) <- c("X=1", "X=2", "X=3")
print(fxy)

x <- rep(1:3, each = 3)
y <- rep(1:3, times = 3)
fxy_vals <- as.vector(fxy)


# Gráfico 3D 
scatter3D(x = x, 
          y = y, 
          z = fxy_vals,
          colvar = NULL, col = "blue",
          pch = 19, cex = 1.5,
          phi = 20, theta = 45,
          zlab = "f(x,y)", xlab = "X", ylab = "Y",
          bty = "b2",
          col.panel = "steelblue",
          col.grid = "darkblue")

# Añadir líneas 
for (i in 1:length(x)) {
  lines3D(x = rep(x[i], 2),
          y = rep(y[i], 2),
          z = c(0, fxy_vals[i]),
          col = "blue",
          lwd = 2,
          add = TRUE)
}
</pre>

```{r, eval=FALSE,include=FALSE}
library(plot3D)

# Valores de fxy, X y Y proporcionados
fxy <- matrix(c(0.05, 0.05, 0.00,
                0.05, 0.10, 0.20,
                0.10, 0.35, 0.10),
              ncol = 3, byrow = FALSE)
colnames(fxy) <- c("Y=1", "Y=2", "Y=3")
rownames(fxy) <- c("X=1", "X=2", "X=3")
print(fxy)

x <- rep(1:3, each = 3)
y <- rep(1:3, times = 3)
fxy_vals <- as.vector(fxy)


# Gráfico 3D 
scatter3D(x = x, 
          y = y, 
          z = fxy_vals,
          colvar = NULL, col = "blue",
          pch = 19, cex = 1.5,
          phi = 20, theta = 45,
          zlab = "f(x,y)", xlab = "X", ylab = "Y",
          bty = "b2",
          col.panel = "steelblue",
          col.grid = "darkblue")

# Añadir líneas 
for (i in 1:length(x)) {
  lines3D(x = rep(x[i], 2),
          y = rep(y[i], 2),
          z = c(0, fxy_vals[i]),
          col = "blue",
          lwd = 2,
          add = TRUE)
}

```


<br/><br/>
<center>
```{r, echo=FALSE, out.width="80%", fig.align = "center"}
knitr::include_graphics("img/plot3Discretas.png")
```
**Figura 2.16** Distribución conjunta discreta-discreta. 
</center>
<br/><br/>

</p>
</div>



</br></br>
<h3>Distribución marginal</h3>

A partir de la **función de distribución conjunta** de dos variables aleatorias, se pueden obtener las **distribuciones marginales**, las cuales describen el comportamiento individual de cada variable, sin considerar la otra. Estas distribuciones se denotan comúnmente como \( g(x) \) para \( X \) y \( h(y) \) para \( Y \).  

Si \( X \) y \( Y \) son **variables aleatorias discretas**, entonces:

- La distribución marginal de \( X \), que representa la probabilidad de cada valor de \( X \), se calcula como:

   \[
   g(x) = f_{X}(x) = \sum_{y=y_{(1)}}^{y_{(n)}} f_{X,Y}(x,y)
   \]

   Esta suma acumula todas las probabilidades conjuntas correspondientes a un valor fijo de \( X \).

- De manera análoga, la distribución marginal de \( Y \) está dada por:

   \[
   h(y) = f_{Y}(y) = \sum_{x=x_{(1)}}^{x_{(n)}} f_{X,Y}(x,y)
   \]

   En este caso, la suma acumula todas las probabilidades conjuntas asociadas a un valor fijo de \( Y \).


</br></br>
<div class="caja-ejemplo">
<h3>Ejemplo:</h3>
<p>

Este ejemplo ilustra cómo obtener las **funciones marginales** \( g(x) \) y \( h(y) \) a partir de la **Tabla 2.7** de distribución conjunta.

La función marginal de \( X \) se obtiene **sumando las probabilidades por columnas**, lo que equivale a acumular todas las probabilidades asociadas a \( Y \) para cada valor de \( X \). El siguiente código en **R** realiza este cálculo:

<pre>
# Valores de fxy, X y Y proporcionados
fxy <- matrix(c(0.05, 0.05, 0.00,
                0.05, 0.10, 0.20,
                0.10, 0.35, 0.10),
              ncol = 3, byrow = FALSE)
colnames(fxy) <- c("Y=1", "Y=2", "Y=3")
rownames(fxy) <- c("X=1", "X=2", "X=3")

# Calcula la suma marginal por filas (suma sobre Y) para obtener g(x)
gx <- addmargins(fxy, 1)

# Asigna nombres a las filas, incluyendo la fila de sumas g(x)
rownames(gx) <- c("1", "2", "3", "g(x)")

# Tabla resultante gx
print(gx)
</pre>


```{r,eval=FALSE,include=FALSE}
# Valores de fxy, X y Y proporcionados
fxy <- matrix(c(0.05, 0.05, 0.00,
                0.05, 0.10, 0.20,
                0.10, 0.35, 0.10),
              ncol = 3, byrow = FALSE)
colnames(fxy) <- c("Y=1", "Y=2", "Y=3")
rownames(fxy) <- c("X=1", "X=2", "X=3")

# Calcula la suma marginal por filas (suma sobre Y) para obtener g(x)
gx <- addmargins(fxy, 1)

# Asigna nombres a las filas, incluyendo la fila de sumas g(x)
rownames(gx) <- c("1", "2", "3", "g(x)")

# Tabla resultante gx
print(gx)
```

La salida computacional muestra los valores de la distribución marginal $g(x)$:

<pre>
      Y=1  Y=2  Y=3
1    0.05 0.05 0.10
2    0.05 0.10 0.35
3    0.00 0.20 0.10
g(x) 0.10 0.35 0.55
</pre>



La función marginal de $Y$ se obtiene sumando las probabilidades por filas, es decir, acumulando todas las probabilidades de $X$ para cada valor de $Y$. El siguiente código en **R** realiza este cálculo:

<pre>
# Valores de fxy, X y Y proporcionados
fxy <- matrix(c(0.05, 0.05, 0.00,
                0.05, 0.10, 0.20,
                0.10, 0.35, 0.10),
              ncol = 3, byrow = FALSE)
colnames(fxy) <- c("Y=1", "Y=2", "Y=3")
rownames(fxy) <- c("X=1", "X=2", "X=3")

# Calcula la suma marginal por columnas (suma sobre X) para obtener h(y)
hy <- addmargins(fxy, 2)

# Asigna nombres a las columnas, incluyendo la columna de sumas h(y)
colnames(hy) <- c("1", "2", "3", "h(y)")

# Mostra la tabla resultante hy
print(hy)
</pre>


```{r, eval=FALSE,include=FALSE}
# Valores de fxy, X y Y proporcionados
fxy <- matrix(c(0.05, 0.05, 0.00,
                0.05, 0.10, 0.20,
                0.10, 0.35, 0.10),
              ncol = 3, byrow = FALSE)
colnames(fxy) <- c("Y=1", "Y=2", "Y=3")
rownames(fxy) <- c("X=1", "X=2", "X=3")

# Calcula la suma marginal por columnas (suma sobre X) para obtener h(y)
hy <- addmargins(fxy, 2)

# Asigna nombres a las columnas, incluyendo la columna de sumas h(y)
colnames(hy) <- c("1", "2", "3", "h(y)")

# Mostra la tabla resultante hy
print(hy)
```

La salida computacional muestra los valores de la distribución marginal $h(y)$:

<pre>
       1    2    3 h(y)
X=1 0.05 0.05 0.10  0.2
X=2 0.05 0.10 0.35  0.5
X=3 0.00 0.20 0.10  0.3
</pre>

</p>
</div>




</br></br>
<h3>Función de probabilidad condicional</h3>

La **función de probabilidad condicional** describe cómo se distribuye la probabilidad de \( X \) cuando se conoce que \( Y \) ha tomado un valor específico \( y_0 \):

\[
f_{X|Y}(x|y_0) =
\begin{cases}
    \dfrac{f_{X,Y}(x,y_0)}{h(y_0)}, & \text{si } h(y_0) > 0 \\
    0, & \text{en otro caso}
\end{cases}
\]

Aquí, \( f_{X|Y}(x|y_0) \) representa la probabilidad de que \( X \) tome el valor \( x \) **dado** que \( Y = y_0 \).   El denominador \( h(y_0) \) corresponde a la **distribución marginal de \( Y \)**.  

<div class="caja-nota">
<h3>Nota:</h3>
>
<p>
- Si \( h(y_0) = 0 \), la probabilidad condicional no está definida.
</p>
- En el caso de una variable discreta, se cumple que:
\[
\sum_{x} f_{X|Y}(x|y_0) = 1
\]
>
</div>



---

De manera análoga, la **función de probabilidad condicional** describe la distribución de \( Y \) cuando se conoce que \( X \) ha tomado un valor específico \( x_0 \):

\[
f_{Y|X}(y|x_0) =
\begin{cases}
    \dfrac{f_{X,Y}(x_0,y)}{g(x_0)}, & \text{si } g(x_0) > 0 \\
    0, & \text{en otro caso}
\end{cases}
\]

En este caso, \( f_{Y|X}(y|x_0) \) representa la **probabilidad condicional** de que \( Y \) tome el valor \( y \) dado que \( X = x_0 \).  
El denominador \( g(x_0) \) corresponde a la **distribución marginal de \( X \)**.  


<div class="caja-nota">
<h3>Nota:</h3>
>
<p>
- Si \( g(x_0) = 0 \), la probabilidad condicional no está definida.
</p>
- Para una variable discreta, se cumple que:
\[
\sum_{y} f_{Y|X}(y|x_0) = 1
\]
>
</div>



</br></br>
<div class="caja-ejemplo">
<h3>Ejemplo:</h3>
<p>
Continuando con el ejemplo de la **Tabla 2.7**, en este ejemplo se muestra el proceso para obtener la función de probabilidad condicional de $X$ dado que $Y=2$.

El valor de la distribución marginal de $Y$ en $y=2$, está determinada por la expresión:
  $$h(y=2) = 0.050 + 0.10 + 0.35 = 0.50$$

La función condicional $f_{X|Y}(x|y_0)$ se calcula como:
$$f_{X|Y}(x|y_0=2) = \frac{f_{X,Y}(x,y=2)}{h(y=2)}$$
La tabla siguiente muestra los resultados de $f_{X|Y}(x|y_0=2)$ para los distintos valores de $X$:

| $x$ | $f_{X|Y}(x|y_0=2)$                     | Cálculo                        | Resultado |
|--------|--------------------------|----------------------------------|----------|
| 1      | $\frac{f(1,2)}{0.50}$  | $\frac{0.05}{0.50}$            | 0.10     |
| 2      | $\frac{f(2,2)}{0.50}$  | $\frac{0.10}{0.50}$            | 0.20     |
| 3      | $\frac{f(3,2)}{0.50}$  | $\frac{0.20}{0.50}$            | 0.40     |

</p>
</div>






</br></br>
<h2>Continuo-continuo</h2>


</br></br>
<h3>Función de densidad conjunta</h3>

Cuando $X$ y $Y$ son **variables aleatorias continuas**, su **función de densidad conjunta** $f_{X,Y}(x,y)$ permite determinar la probabilidad de que estas variables tomen valores dentro de una región específica $R$.


La probabilidad de que $(X, Y)$ se encuentren dentro de la región $R$ está dada por:
$$
P((X,Y) \in R) = \int\int_{R} f_{X,Y}(x,y) \,dx\,dy
$$
Esta integral representa el **volumen** bajo la superficie $f_{X,Y}(x,y)$ sobre la región $R$.

</br></br>
<h3>Propiedades de la función de densidad conjunta $f_{X,Y}(x,y)$</h3>

Para que $f_{X,Y}(x,y)$ sea válida como densidad de probabilidad, debe cumplir:

1. **No negatividad:** La densidad siempre es positiva o nula:
   $$f(x,y) \geq 0, \quad \forall (x,y)$$

2. **Totalidad:** La integral doble sobre todo el espacio es 1:
   $$
   \int_{-\infty}^{\infty} \int_{-\infty}^{\infty} f_{X,Y}(x,y) \,dx\,dy = 1
   $$
Es importante tener en cuenta que $f_{X,Y}(x,y)$ no es una probabilidad, sino una densidad; la probabilidad se obtiene integrando. El área bajo la superficie en cualquier región específica es la probabilidad de que $(X, Y)$ esté en esa región. La integral total bajo toda la superficie es **1**, garantizando que cubre todo el espacio de posibilidades.


</br></br>
<div class="caja-ejemplo">
<h3>Ejemplo:</h3>
<p>
Se modelan la proporción de ácido $X$ y de ácido $Y$ (en litros) en una mezcla mediante la función de densidad conjunta $f_{X,Y}(x,y)$ definida como:

$$
f_{X,Y}(x,y) = \left\{ \begin{matrix} 
  (x+y) & \text{si } 0 \leq x \leq 1 \text{ y } 0 \leq y \leq 1 \\\\
  0 & \text{en otro caso}
\end{matrix} \right.
$$


Para verificar que $f_{X,Y}(x,y)$ es una función de densidad válida, la integral doble sobre la región definida debe ser igual a 1:

$$
\int_{0}^{1} \int_{0}^{1} (x+y) \,dx\,dy
$$

Integración respecto a $x$:

$$
\int_{0}^{1} (x+y) \,dx = \left( \frac{x^2}{2} + yx \right) \Bigg|_{0}^{1} = \frac{1}{2} + y
$$

Integración respecto a $y$:

$$
\int_{0}^{1} \left( \frac{1}{2} + y \right) \,dy = \left( \frac{1}{2}y + \frac{y^2}{2} \right) \Bigg|_{0}^{1}
= \frac{1}{2} + \frac{1}{2} = 1
$$
El valor de la integral doble es **1**, confirmando que $f_{X,Y}(x,y)$ es una **función de densidad conjunta válida**.

Se puede usar la librería `cubature` para integrar numéricamente la función conjunta $f_{X,Y}(x,y) = x + y$ en el intervalo $[0,1] \times [0,1]$ como se muestra a continuación:


<pre>
# Cargar la librería 'cubature' para realizar la integración numérica
library(cubature)

# Define la función conjunta f(x,y) = x + y
fxy <- function(x) {
  return(x[1] + x[2])  # x[1] = x, x[2] = y
}

# Realiza la integración numérica en el intervalo [0,1] para x y y
Ifxy <- adaptIntegrate(
  fxy,
  lowerLimit = c(0, 0),  # Límites inferiores para x e y
  upperLimit = c(1, 1)   # Límites superiores para x e y
)

# Muestra el valor de la integral calculada
Ifxy$integral
</pre>



```{r, eval=FALSE,include=FALSE}
# Cargar la librería 'cubature' para realizar la integración numérica
library(cubature)

# Define la función conjunta f(x,y) = x + y
fxy <- function(x) {
  return(x[1] + x[2])  # x[1] = x, x[2] = y
}

# Realiza la integración numérica en el intervalo [0,1] para x y y
Ifxy <- adaptIntegrate(
  fxy,
  lowerLimit = c(0, 0),  # Límites inferiores para x e y
  upperLimit = c(1, 1)   # Límites superiores para x e y
)

# Muestra el valor de la integral calculada
# Ifxy$integral
```


La función conjunta $f_{X,Y}(x,y) = x + y$ se puede graficar como en la **Figura 2.17** con los códigos siguientes:

<pre>
library(plot3D)

# Definir la función conjunta
density_function <- function(x, y) {
  ifelse(x >= 0 & x <= 1 & y >= 0 & y <= 1, x + y, 0)
}

# Crear una cuadrícula de valores
gx <- seq(0, 1, length.out = 30)
gy <- seq(0, 1, length.out = 30)
g <- expand.grid(x = gx, y = gy)
g$z <- mapply(density_function, g$x, g$y)

# Crear el gráfico 3D
persp3D(
  x = gx, 
  y = gy, 
  z = matrix(g$z, nrow = 30),
  col = "lightblue", 
  theta = 45, 
  phi = 30, 
  xlab = "X", 
  ylab = "Y", 
  zlab = "f(X,Y)",
  main = ""
)
</pre>

```{r, eval=FALSE,include=FALSE}
library(plot3D)

# Definir la función conjunta
density_function <- function(x, y) {
  ifelse(x >= 0 & x <= 1 & y >= 0 & y <= 1, x + y, 0)
}

# Crear una cuadrícula de valores
gx <- seq(0, 1, length.out = 30)
gy <- seq(0, 1, length.out = 30)
g <- expand.grid(x = gx, y = gy)
g$z <- mapply(density_function, g$x, g$y)

# Crear el gráfico 3D
persp3D(
  x = gx, 
  y = gy, 
  z = matrix(g$z, nrow = 30),
  col = "lightblue", 
  theta = 45, 
  phi = 30, 
  xlab = "X", 
  ylab = "Y", 
  zlab = "f(X,Y)",
  main = ""
)
```


```{r, echo=FALSE, out.width="80%", fig.align = "center"}
knitr::include_graphics("img/Figura318.png")
```
**Figura 2.17** Distribución conjunta $f_{X,Y}(x,y) = x + y$ para $0 \leq x \leq 1$ y $0 \leq y \leq 1$.
</center>

</p>
</div>



</br></br>
<h3>Función de densidad marginal </h3>

Cuando \( X \) y \( Y \) son **variables aleatorias continuas**, sus distribuciones marginales se obtienen **integrando la función de densidad conjunta**.

La **función de densidad marginal de \( X \)** se define como:

\[
g(x) = f_{X}(x) = \int_{-\infty}^{\infty} f_{X,Y}(x,y) \,dy
\]

Esta integral suma las contribuciones de la densidad conjunta para todos los posibles valores de \( Y \), obteniendo así la distribución marginal de \( X \).

De manera análoga, la **función de densidad marginal de \( Y \)** se expresa como:

\[
h(y) = f_{Y}(y) = \int_{-\infty}^{\infty} f_{X,Y}(x,y) \,dx
\]

En este caso, la integral acumula la densidad conjunta sobre todos los valores de \( X \), obteniendo así la distribución marginal de \( Y \).  




</br></br>
<div class="caja-ejemplo">
<h3>Ejemplo:</h3>
<p>

Continuando con el ejemplo anterior, la **función de densidad marginal de \( X \)**, denotada por \( g(x) \), se obtiene integrando la función de densidad conjunta respecto a \( y \):

\[
g(x) = \int_{0}^{1} (x + y) \, dy = \left( xy + \frac{y^{2}}{2} \right) \Bigg|_{0}^{1} = x + \frac{1}{2}
\]

Por lo tanto, la expresión completa de la densidad marginal de \( X \) es:

\[
g(x) =
\begin{cases}
x + \dfrac{1}{2}, & \text{si } 0 \leq x \leq 1 \\
0, & \text{en otro caso}
\end{cases}
\]

De forma análoga, la **función de densidad marginal de \( Y \)**, denotada por \( h(y) \), se obtiene integrando la densidad conjunta respecto a \( x \):

\[
h(y) = \int_{0}^{1} (x + y) \, dx = \left( \frac{x^2}{2} + yx \right) \Bigg|_{0}^{1} = \frac{1}{2} + y
\]

Así, la expresión completa de la densidad marginal de \( Y \) es:

\[
h(y) =
\begin{cases}
y + \dfrac{1}{2}, & \text{si } 0 \leq y \leq 1 \\
0, & \text{en otro caso}
\end{cases}
\]

> **Nota:** Aunque las fórmulas para las funciones marginales se definen como integrales en todo el dominio (\(-\infty\) a \(\infty\)), en este ejemplo las funciones se anulan fuera del intervalo \([0,1]\) porque la densidad conjunta \( f_{X,Y}(x,y) = x + y \) está definida solo en el cuadrado \( 0 \leq x \leq 1 \), \( 0 \leq y \leq 1 \).

</p>
</div>



</br></br>
<h3>Función de densidad condicionales</h3>


La **función de densidad condicional de \( X \)** dado que \( Y = y_0 \) se define como:

\[
f_{X|Y}(x|y_0) =
\begin{cases}
\displaystyle\frac{f_{X,Y}(x, y_0)}{h(y_0)}, & \text{si } h(y_0) > 0 \\
0, & \text{en otro caso}
\end{cases}
\]

De manera análoga, la **función de densidad condicional de \( Y \)** dado que \( X = x_0 \) se expresa como:

\[
f_{Y|X}(y|x_0) =
\begin{cases}
\displaystyle\frac{f_{X,Y}(x_0, y)}{g(x_0)}, & \text{si } g(x_0) > 0 \\
0, & \text{en otro caso}
\end{cases}
\]

Estas funciones describen cómo se distribuye una variable continua cuando se conoce el valor de la otra. En ambos casos, el denominador corresponde a la **densidad marginal** de la variable condicionante, y actúa como un factor de normalización para asegurar que la densidad condicional se integre a 1 sobre su dominio.



</br></br>
<h3>Covarianza</h3>

La **covarianza** cuantifica la **relación lineal** entre dos variables aleatorias \( X \) y \( Y \), y se define como la esperanza del producto de sus desviaciones respecto a sus medias:

\[
\text{Cov}[X,Y] = E[(X - E[X])(Y - E[Y])]
\]

Esta expresión mide cómo varían conjuntamente \( X \) y \( Y \) alrededor de sus valores esperados. Una forma algebraicamente equivalente de calcular la covarianza es:

\[
\text{Cov}[X,Y] = E[XY] - E[X]E[Y]
\]


</br></br>
<h4>Cálculo de \( E[XY] \) según el tipo de variables</h4>


- **Caso discreto-discreto:** 

  Si \( X \) y \( Y \) son variables aleatorias discretas, el valor esperado del producto se calcula mediante una **suma doble** sobre sus respectivos recorridos:

  \[
  E[XY] = \sum_{x \in R_X} \sum_{y \in R_Y} xy \, f_{X,Y}(x, y)
  \]

- **Caso continuo-continuo:**  

  Si \( X \) y \( Y \) son variables continuas, se utiliza una **integral doble** sobre todo el dominio de la función de densidad conjunta:

  \[
  E[XY] = \int_{-\infty}^{\infty} \int_{-\infty}^{\infty} xy \, f_{X,Y}(x, y) \, dx \, dy
  \]

</br></br>
<h4>Interpretación de la covarianza</h4>


- \( \text{Cov}[X,Y] > 0 \): Existe una **asociación lineal positiva**, es decir, cuando \( X \) aumenta, \( Y \) tiende a aumentar también.

- \( \text{Cov}[X,Y] < 0 \): Existe una **asociación lineal negativa**, es decir, cuando \( X \) aumenta, \( Y \) tiende a disminuir.

- \( \text{Cov}[X,Y] = 0 \): No hay evidencia de una relación lineal entre \( X \) y \( Y \). Sin embargo, podrían estar relacionadas de manera no lineal.

> **Nota:** Aunque la covarianza indica la dirección de la asociación lineal, su magnitud **depende de las unidades de las variables** y por tanto no es comparable entre diferentes contextos. Para una medida estandarizada se utiliza el **coeficiente de correlación de Pearson**.





</br></br>
<h3>Coeficiente de correlación de Pearson ($\rho$)</h3>

El **coeficiente de correlación de Pearson** (\( \rho \)) cuantifica la **fuerza y dirección** de la asociación lineal entre dos variables aleatorias \( X \) y \( Y \). Se define como la covarianza entre ambas variables, normalizada por el producto de sus desviaciones estándar:

\[
\rho = \frac{\text{Cov}[X,Y]}{\sqrt{\text{Var}(X) \cdot \text{Var}(Y)}}
\]

Esta medida es **adimensional** y siempre está acotada en el intervalo:

\[
-1 \leq \rho \leq 1
\]


</br></br>
<h4>Interpretación del coeficiente \( \rho \)</h4>


- \( \rho = 1 \): Existe una **relación lineal positiva perfecta**. A medida que \( X \) aumenta, \( Y \) también lo hace proporcionalmente.

- \( \rho = -1 \): Existe una **relación lineal negativa perfecta**. A medida que \( X \) aumenta, \( Y \) disminuye proporcionalmente.

- \( \rho = 0 \): **No hay relación lineal** entre \( X \) y \( Y \). Sin embargo, podrían estar relacionadas de forma no lineal.

- **Valores intermedios** (por ejemplo, \( \rho = 0.6 \), \( \rho = -0.8 \)) indican distintos grados de **asociación lineal**, siendo más fuerte cuanto más cercano esté \( \rho \) a \(\pm 1\).

> **Nota:** A diferencia de la covarianza, el coeficiente de correlación permite comparar la fuerza de la relación lineal entre variables de diferentes escalas o unidades.


La **Tabla 2.8**  presenta una pauta para interpertar los valores de $\rho$.



<br/><br/>
<center>
**Tabla 2.8** Grados de asociación lineal según $\rho$.
</center>


| **Rango de $\rho$**              | **Grado de asociación lineal**         |
|-------------------------------------|-------------------------------|
| $-1.00 \leq \rho < -0.90$         | Negativa muy fuerte          |
| $-0.90 \leq \rho < -0.75$         | Negativa considerable        |
| $-0.75 \leq \rho < -0.50$         | Negativa media              |
| $-0.50 \leq \rho < -0.25$         | Negativa débil              |
| $-0.25 \leq \rho < -0.10$         | Negativa muy débil          |
| $-0.10 \leq \rho < 0.10$         | No existe correlación       |
| $0.10  \leq  \rho < 0.25$           | Positiva muy débil          |
| $0.25  \leq  \rho < 0.50$           | Positiva débil              |
| $0.50  \leq  \rho < 0.75$           | Positiva media              |
| $0.75  \leq  \rho < 0.90$           | Positiva considerable       |
| $0.90  \leq  \rho \leq 1.00$        | Positiva muy fuerte         |


Los gráficos de  la **Figura 2.18** muestran la relación entre dos variables para diferentes valores del coeficiente de correlación de Pearson ($\rho$), destacando cómo varía la asociación lineal negativa:


- **$\rho = -1.0$ (a):** Relación lineal negativa exacta; todos los puntos se alinean sobre una recta descendente.

- **$\rho = -0.90$ (b):** Relación negativa lineal casi perfecta, con una ligera 
dispersión alrededor de una línea descendente.

- **$\rho = -0.75$ (c):** Patrón descendente lineal claro, aunque con más dispersión que en $\rho = -0.90$.

- **$\rho = -0.50$ (d):** Relación lineal negativa moderada, con una nube de puntos más dispersa pero con tendencia descendente.

- **$\rho = -0.25$ (e):** Relación lineal negativa leve; la tendencia descendente es poco perceptible.

- **$\rho = 0.0$ (f):** No se observa un patrón lineal; la distribución de puntos es aleatoria.

A medida que $\rho$ se acerca a -1, la relación negativa lineal es más fuerte y los puntos están más alineados. Cuando $\rho$ se aproxima a 0, la relación lineal desaparece, y la dispersión es aleatoria.

<center>
```{r, echo=FALSE, out.width="100%", fig.align = "center"}
knitr::include_graphics("img/Rho1.png")
```
**Figura 2. 18**  Correlaciones negativas (a) $\rho = -1.0$. $\hspace{.5cm}$ (b) $\rho = -0.90$. <br/> (c) $\rho = -0.75$.  $\hspace{.5cm}$(d) $\rho = -0.50$. $\hspace{.5cm}$ (e) $\rho = -0.25$. $\hspace{.5cm}$ (f) $\rho = 0.0$.
</center>

<br/><br/>
Los gráficos de  la **Figura 2.19** muestran la relación entre dos variables para diferentes valores del coeficiente de correlación de Pearson ($\rho$), destacando cómo varía la asociación lineal positiva.

- **$\rho = 0.10$ (a):** Relación positiva lineal apenas perceptible; los puntos están dispersos.

- **$\rho = 0.25$ (b):** Tendencia ligeramente lineal ascendente, pero con considerable dispersión.

- **$\rho = 0.50$ (c):** Relación positiva lineal más clara; los puntos tienden a alinearse en una dirección ascendente.

- **$\rho = 0.75$ (d):** Tendencia ascendente lineal marcada; los puntos están más próximos a una línea.

- **$\rho = 0.90$ (e):** Relación casi lineal; los puntos forman una franja muy estrecha en dirección ascendente.

- **$\rho = 1.0$ (f):** Relación lineal perfecta; todos los puntos están alineados sobre una recta ascendente.

A medida que $\rho$ se acerca a 1, la relación positiva lineal es más fuerte y los puntos están más alineados. Cuando $\rho$ es bajo, la tendencia lineal positiva es leve, y la dispersión es mayor.

<center>
```{r, echo=FALSE, out.width="100%", fig.align = "center"}
knitr::include_graphics("img/Rho2.png")
```
**Figura 2. 19**  Correlaciones positivas  (a) $\rho = 0.10$.$\hspace{.5cm}$ (b) $\rho = 0.25$.  <br/> (c) $\rho = 0.50$.$\hspace{.5cm}$  (d) $\rho = 0.75$. $\hspace{.5cm}$ (e) $\rho = 0.90$. $\hspace{.5cm}$ (f) $\rho = 1.0$.
</center>



</br></br>
<h2>Independencia</h2>


Sean \( X \) y \( Y \) dos **variables aleatorias**, ya sean discretas o continuas, con:

- **Función de probabilidad (o densidad) conjunta:** \( f_{X,Y}(x, y) \)

- **Funciones marginales:** \( g(x) \) para \( X \) y \( h(y) \) para \( Y \)

Se dice que \( X \) y \( Y \) son **estadísticamente independientes** si, y solo si, para todo par de valores \( x \) y \( y \):

\[
f_{X,Y}(x, y) = g(x) \cdot h(y)
\]

Es decir, la **función conjunta** se puede descomponer como el **producto de las funciones marginales**. Esto implica que el conocimiento del valor de una variable **no proporciona información** sobre la otra: el comportamiento de una es **completamente independiente** del comportamiento de la otra.



</br>
<h4>Propiedades clave:</h4>


- **Relación con la esperanza:** 

  Si \( X \) y \( Y \) son independientes, entonces se cumple que:
  \[
  E[XY] = E[X] \cdot E[Y]
  \]

- **Relación con la covarianza:**  

  La independencia implica covarianza nula:
  \[
  \text{Cov}(X, Y) = 0
  \]
  Sin embargo, el recíproco **no siempre es cierto**: una covarianza igual a cero **no garantiza independencia**, ya que podría existir una relación no lineal entre las variables.



</br></br>
<div class="caja-ejemplo">
<h3>Ejemplo:</h3>
<p>

Continuando con el ejemplo anterior, la **función de densidad conjunta** está dada por:

\[
f_{X,Y}(x,y) =
\begin{cases}
x + y, & \text{si } 0 \leq x \leq 1 \text{ y } 0 \leq y \leq 1 \\
0, & \text{en otro caso}
\end{cases}
\]

Las **funciones de densidad marginal** de \( X \) y \( Y \) son:

\[
g(x) =
\begin{cases}
x + \dfrac{1}{2}, & \text{si } 0 \leq x \leq 1 \\
0, & \text{en otro caso}
\end{cases}
\]

\[
h(y) =
\begin{cases}
y + \dfrac{1}{2}, & \text{si } 0 \leq y \leq 1 \\
0, & \text{en otro caso}
\end{cases}
\]

Para que \( X \) y \( Y \) sean **estadísticamente independientes**, debe cumplirse que:

\[
f_{X,Y}(x,y) = g(x) \cdot h(y)
\]

Primero se calcula el producto de las densidades marginales:

\[
g(x) \cdot h(y) = (x + \tfrac{1}{2})(y + \tfrac{1}{2}) = xy + \frac{x}{2} + \frac{y}{2} + \frac{1}{4}
\]

Ahora de compara este resultado con la densidad conjunta:

\[
f_{X,Y}(x,y) = x + y
\]

Claramente:

\[
x + y \neq xy + \frac{x}{2} + \frac{y}{2} + \frac{1}{4}
\]

Por lo tanto, como:

\[
f_{X,Y}(x,y) \neq g(x) \cdot h(y),
\]

se concluye que **las variables \( X \) y \( Y \) no son independientes**.


</p>
</div>